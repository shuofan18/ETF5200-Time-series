---
title: "ETF5200 Applied time series econometrics"
subtitle: "Project 2"
author: "Shuofan Zhang 27886913"
bibliography: reference.bib
date: ''
fontsize: 12pt
output: 
  pdf_document: 
    latex_engine: xelatex
link-citations: yes
delete_merged_file: yes
biblio-style: apalike
toc: yes
---
```{r libs, echo=FALSE, message=FALSE, warning=FALSE}
library(tsDyn)
library(mvtnorm)
library(vars)
library(knitr)
library(lmtest)
library(readxl)
library(tidyverse)
library(reshape2)
```

# Part I

Personal capita consumption and income data are more likely to be inter-dependent dynamically, therefore vector autoregressive model (VAR) will be used to study their behavior in this project. By looking at the data summary, we can see we have 251 observations, there are no missing values. The period of this data set is from the first quarter 1947 to the third quarter 2009. All data is in log form. All graphs and tables are in appendix.

## Question 1

We start with checking the time plots of these two series, which are shown in figure 1. We can see the increasing time trend clearly. The VAR model needs to be stationary. We first difference the series. The graph after first difference is shown in figure 2, we can see the first-differenced series are stationary. Hence the interested model form is as below:

$$\Delta Y_t=A_0 + A_1 \Delta Y_{t-1} + A_2 \Delta Y_{t-2} + \epsilon_t$$
where 
$$\Delta Y_t=\begin{pmatrix}
  \Delta Consumption_t \\
  \Delta Income_t
 \end{pmatrix} \ \ 
 \epsilon_t=\begin{pmatrix}
  \epsilon_{1t} \\
  \epsilon_{2t}
 \end{pmatrix}$$
 
and $A_0$, $A_1$, $A_2$ are coefficient matrices.

First we want to estimate the intercept vector ($A_0$) and the coefficient matrices ($A_1$ and $A_2$) for the VAR(2) model. The coefficient matrices for the first lag and the second lag are shown in table 1 and table 2 respectively.

The intercept matrix is shown in table 3.

## Question 2

AIC, HQ and FPE choose lag of order 3 while BIC chooses 1. Since BIC tends to under-estimate, All other three IC chooses 3 lags, we will consider 3 lags for the first-differenced data as the correct order p in this case.

```{r table1, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, results='asis'}
biv <- read_excel("USdata2.xls", sheet = 1, range = "A1:B252") %>% ts(start=c(1947,1), frequency = 4)
dfbiv <- diff(biv) 
model <- VAR(dfbiv, p=2, type="const")
Acm = Acoef(model)
A1 <- Acm[[1]]
A2 <- Acm[[2]]
kable(A1, col.names = c("Consumption lag 1", "Income lag 1"), caption = "Coefficient matrix A1")
A0 <- matrix(c(0.044288507, 0.001650857), ncol=1)
A0 <- cbind(c("Consumption", "Income"), A0) %>% as.data.frame()
```


```{r table2, echo=F, message=FALSE, warning=FALSE, paged.print=FALSE, results='asis'}
kable(A2, col.names = c("Consumption lag 2", "Income lag 2"), caption = "Coefficient matrix A2")
kable(A0, col.names = c("","Intercept"), caption = "Intercept matrix")
```


```{r ic, echo=F, message=FALSE, warning=FALSE, paged.print=FALSE, results='asis'}
AIC <- VAR(dfbiv, lag.max = 10, ic="AIC", type = "const") %>% .$p
BIC <- VAR(dfbiv, lag.max = 10, ic="SC", type = "const") %>% .$p
HQ <- VAR(dfbiv, lag.max = 10, ic="HQ", type = "const") %>% .$p
FPE <- VAR(dfbiv, lag.max = 10, ic="FPE", type = "const") %>% .$p
IC <- c(AIC, BIC, HQ, FPE)
kable(IC, col.names = "Lags chosen by Information Criteria", caption = "VAR with first-differenced data. AIC, HQ and FPE choose lag of order 3 while BIC chooses 1.")
```

# Part II

## The main ideas proposed

In 1981, Granger first introduced the idea of co-integration in @G1981. He believed that "the practice of data-mine should be brought out into the open" and this co-integration idea was "potentially very important". He was curious about if prices and wages were co-integrated or not, and he suggested appropriate investigation was needed to find out which pairs of economic variables are co-integrated. [@G1981] In 1983, Granger and Weiss did further study on this topic. "A theorem showing precisely that co-integration series can be represented by error correction models was originally stated and proved." [@E1987] Some error-correction model forms and testing methods were discussed, three applications were conducted which including employees' income and national income; M3 and GNP; prices, wages and productivity. However the results in the three applications in that paper were all unsatisfactory. [@G1983]

Therefore, in 1987, Engle and Granger wrote this paper [@E1987], which becomes very influential in multivariate time series analysis later on, to formally define the conception error-correcting models and co-integration. 

They found that several forms can be used in a co-integration system, each of the representations has associated restrictions. "It was once thought that the best way to model multivariate time series with unit roots was to difference them first and then use the VAR machinery to develop a multivariate time series model for the differenced variables." [@F2017] However, in this paper, @E1987, the authors concluded that a vector autoregression (VAR) in differenced variables will be misspecified if the variables are co-integrated. A vector autoregression (VAR) with level co-integrated data will have omitted important constraints. 

Properties of co-integration varibles associated with these representations were discussed and proved in this paper. Among different representations, the error correction form (VECM) was found to be most convenient to be estimated. The maximum likelihood estimator with an iterative procedure is a suitable estimator. What's more, a two-step estimator was proposed which was proved to have excellent properties based on certain condition. Seven test statistics were formulated and analyzed for testing for co-integration. 

## The main techniques used  

Some mathematical theorems and linear algebra calculation were employed to prove the statement made in the paper. We will maily discuss two of them in details below.

### Wold theorem  

Suppose that $x_t$ is a covariance stationary process with $E(x_t)=0$ and covariance function, $\gamma(j)=E(x_t x_{t-j}) \ \forall j$. Then $$x_t=\sum_{j=0}^\infty d_j \epsilon_{t-j}+\eta_t$$ where $d_0=1,\ \sum_{j=0}^\infty d_j^2 <0, E(\epsilon_t^2)=\sigma_\epsilon^2,\ E(\epsilon_t \epsilon_s)=0 \  for\ t\ne s$ and $E(\epsilon_t)=0,\ E(\eta_t\epsilon_s)=0\ for\ all\ t,s$ and $P[\eta_{t+s}|x_{t-1},x_{t-2},...]=\eta_{t+s},\ s\geq0$
This theorem can be applied to multivariate time series as well. When $x_t$ is a vector of I(1) variables, equation 3.1 can be derived directly from this theorem.
$$(1-B)x_t=C(B)\epsilon_t$$

### Lemma of the moving average polynomial  

Lemma: Any polynomial $C(L)$ of degree k can be written as the sum of $C(1)L^j\ \ (J\in{0,1,...,K})$ and another polynomial of degree k that has a unit root. In this paper, they used this lemma with $j=1$. So the moving average polynomial $C(B)$ is expressed as 
$$C(B)=C(1)+(1-B)C^*(B)$$
With the two theorem introduced above, the representation of VECM can be derived. 
$$A^*(B)(1-B)x_t=-\gamma z_{t-1}+d(B)\epsilon_t\ \ \ \ \ \ \ (3.4)$$ 
It turned out later in this paper that this form provides a very convenient way of being estimated.

In terms of estimation, maximum likelihood estimator with an iterative procedure can be used, a two step estimator was also proposed. This two-step estimator require only single equation least squares and is consistent for all the parameters.

As to testing, seven test statistics were formulated and analyzed for testing for co-integration. Monte Carlo simulation was used to calculate the relevant critical values.

## The main data used  

Four examples were given to show the performance of the tests introduced in this paper. Thus four types of data were involved.

- The relationship between consumption and income in U.S.
The U.S. quarterly real per capita consumption on nondurables and real per capita disposable income from 1947-I to 1981-II.

- Monthly wages and prices in U.S.
Logs of the consumer price index and production worker wage in manufacturing over the three decades of 50's, 60's, and 70's. For each of the decades there are 120 observations.

- Short and long term interest rates in U.S.
Monthly yields to maturity of 20 years treasury bonds (long term rate $R_t$) and the one month treasury bill rate (short rate $r_t$) from February, 1952 to December, 1982.

- Money supply and Output
Four meansure of money: M1, M2, and M3, and L, total liquid assests. In each case, the sample period was 1959-I through 1981-II, quarterly. 

## The main results obtained  

I see the theorem and methods proposed in this paper as the most significant results obtained by the authors. This includes the Granger Representation Theorem, the two-step estimator and the seven test statistics for testing co-integration. What's more, a series of examples in this paper were conducted with real data, four economic relationships were tested for co-integration. These results are also important results provided by this paper. 

The Granger Representation Theorem establishes the relationship between co-integration and VECM. It presents several representations for co-integrated systems including an autoregression representation and an error-correction represenatation (VECM).
The auroregression representation is:
$$A(B)x_t=d(B)\epsilon_t\ \ \ \ \ \ \ (3.3)$$ where $A(1)$ has rank r (r is the co-integrating rank) and $d(B)$ is a scalar lag polynomial with $d(1)$ finite, and $A(0)=I_N$.
The VECM representation is:
$$A^*(B)(1-B)x_t=-\gamma z_{t-1}+d(B)\epsilon_t\ \ \ \ \ \ \ (3.4)$$ where $A^*(0)=I_N$ and $z_t=\alpha'x_t$ an $r\times 1$ vector of stationary random variables.

What's more, a two step estimator was proposed and proven to be as efficient as MLE based on a known value of co-integrating vector ($\alpha$).

The seven testing statistics are: 
- CRDW statistic is the same with Durbin Watson. If the residuals from co-integrating regression are nonstationary, this statistic will be close to zero. Therefore, the test rejects non-co-integration if DW is too big.

- DF statistic employs Dickey Fuller test by running an auxiliary regression for the residuals from the co-integrating regression.

- ADF statistic is the augmented Dickey-Fuller test, it allows for more dynamics.

- RVAR test is testing whether the error-correction term is significant in the error-correction model. And it is based on the sum of the squared t statistics.

- ARVA, when a higher order system is postulated, ARVA test is employed.

- UVAR test statistics uses the F test, given the factor that the model can be adequately expressed entirely in changes if co-integration constrains are not present.

- AUVAR is an augmented or higher order version of the above test.

Through a series of examples, the writter concluded that consumption and income are co-integrated, wages and prices are not, short and long interest rates are, and nomial GNP is co-integrated with M2, but not M1, M3, or aggregate liquid assets. This also solved the question asked by Granger in @G1981.

## The conclusion made  

One of the most influential conclusions made by this paper is that I(1) variables had a vector error correction representation if and only if they were cointegrated.

In the four examples mentioned above, "it is found that consumption and income are co-integrated, wages and prices are not, short and long interest rates are, and nominal GNP is not co-integrated with M1, M3, or total liquid assets, although it is possibly with M2." [@E1987]

## My own models and proposals  

Based on this paper and what we have learned from our ETF5200 subject, the appropraite way of modelling cointegrated I(1) variables can be conducted as:

- Determing the lag length. The lag length can be determined from a VAR in levels using IC. 

- Determining the cointegrating rank. This step can be done by the Johansen method. It tests the rank of $\Pi$. 

- Estimating the VECM.

We will give an example using the data set in part I to illustrate the procedures listed above. Firstly, we use VAR to determine lags, three of IC chooses lags of 4, while only BIC chooses 2, so we will use 4 lags. Secondly we use Johansen test to determine the rank. The null hypothesis $r=0$ can be rejected at 10% significance level, while $r\leq1$ cannot be rejected. We will take $r=1$ as true in this case. 

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, results='asis'}
mm <- read_excel("USdata2.xls", sheet = 1, range = "A1:B252") %>% ts(start=c(1947,1), frequency = 4)
A <- VAR(mm, lag.max = 10, ic="AIC", type = "trend") %>% .$p
B <- VAR(mm, lag.max = 10, ic="SC", type = "trend") %>% .$p
H <- VAR(mm, lag.max = 10, ic="HQ", type = "trend") %>% .$p
FP <- VAR(mm, lag.max = 10, ic="FPE", type = "trend") %>% .$p
IC2 <- c(A, B, H, FP)
kable(IC2, col.names = "Lags chosen by Information Criteria", caption = "VAR with level data. AIC, HQ and FPE choose lag of order 4 while BIC chooses 2.")

jotest <- ca.jo(mm, type = "trace", K=4, ecdet = "trend")
jotest
mmodel <- VECM(mm, lag=3, r=1)
coeA <- mmodel %>% coefA()
coeB <- mmodel %>% coefB()
coeP <- mmodel %>% coefPI()
kable(coeA, col.names = "ECT", caption = "Estimated Coefficient of Error-correction term")
kable(coeB, col.names = "Cointegrating vector", caption = "Estimated Cointegrating vector")
kable(coeP, col.names = c("Consumption", "Income"), caption = "Estimated Pi matrix")
```

## Appendix  

```{r fig1, echo=FALSE, fig.cap="Time plots of personal capita consumption and income shows clear trend. Time plots of first-differenced personal capita consumption and first-differenced income do not have systematic patterns.", message=FALSE, warning=FALSE, paged.print=FALSE}
setwd("/Users/stanza/documents/github/etf5200-time-series")
tsdata <- read_excel("USdata2.xls", sheet = 1, range = "A1:B252") %>% ts(start=c(1947,1), frequency = 4) %>% melt(id="time")

ggplot(tsdata,aes(x=Var1,y=value,colour=Var2,group=Var2))+geom_line()+theme(axis.text.x=element_blank(),axis.text.y=element_blank())+xlab("time")+ylab("level series")+labs(colour = "Variables")

dfdata <- read_excel("USdata2.xls", sheet = 1, range = "A1:B252") %>% ts(start=c(1947,1), frequency = 4)
dfdata <- diff(dfdata) %>% melt(id="time")
ggplot(dfdata,aes(x=Var1,y=value,colour=Var2,group=Var2))+geom_line()+theme(axis.text.x=element_blank(),axis.text.y=element_blank())+xlab("time")+ylab("First-differenced series")+labs(colour = "Variables")+facet_grid(. ~ Var2)
```







## References
