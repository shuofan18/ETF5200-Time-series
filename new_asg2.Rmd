---
title: "ETF5200 Applied time series econometrics"
subtitle: "Project 1"
author: "Shuofan Zhang 27886913"
bibliography: reference.bib
date: ''
fontsize: 12pt
output:
  pdf_document: default
link-citations: yes
delete_merged_file: yes
biblio-style: apalike
toc: yes
---

#Part I  

In this section, we investigate data generating process of four data series of 1960-2009 from the Bureau of Economic Analysis. 
$c_t$ is the logarithm of the per-capital consumption expenditure, $i_t$ is the logarithm of the per-capital disposable income, $p_t$ is the logarithm of GDP, $r_t$ is the real interest rate. All numbers are rounded to four decimal places.

##Question 1  

For each of the data sets, we calculate the OLS estimates of $\alpha$, $\beta$, $\gamma$ and $\sigma_u^2$ assuming the model specified below is correct.
$$\Delta y_t=\alpha+\beta t+\gamma y_{t-1}+u_t$$

```{r data, include=FALSE}
setwd("/Users/stanza/documents/github/etf5200-time-series")
library(tidyverse)
library(forecast)
library(readxl)
library(lmtest)
library(ggfortify)
library(tseries)

data <- read_excel("newdata.xlsx", sheet=1, range=cell_rows(9:261),  na="n/a")

names(data)<-c("year","season", "i_t","c_t","r_t","g_t")
ct <- log(data$c_t)
it <- data$i_t
it <- log(it)
pt <- log(data$g_t)
rt <- data$r_t %>% as.character() %>% as.numeric()


model <- function(x){
  x <- na.omit(x)
  lag <- lag(x)
  lag <- lag[-1]
  t <- c(1:length(lag))
  diff <- diff(x)
  dt <- tibble(diff, t, lag)
  mm <- lm( diff ~ t + lag ,data = dt)
  sigma <- sigma(mm)
  variance <- format(round(sigma^2, digits = 4), scientific = FALSE)
  alpha <- format(round(mm$coefficients[1], digits = 4), scientific=FALSE)
  beta <- format(round(mm$coefficients[2], digits = 4), scientific=FALSE)
  gamma <- format(round(mm$coefficients[3], digits = 4), scientific=FALSE)
  ss <- summary(mm)
  p_alpha <- paste0("(", format(round(ss$coefficients[,4][1],digits=4), scientific=FALSE), ")")
  p_beta <- paste0("(", format(round(ss$coefficients[,4][2],digits=4), scientific=FALSE), ")")
  p_gamma <- paste0("(", format(round(ss$coefficients[,4][3],digits=4), scientific=FALSE), ")")
  tibble(variance,alpha,beta,gamma) %>% rbind(c(" ", p_alpha, p_beta, p_gamma)) %>% return()
}

```

```{r model, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
ctm <- model(ct)
itm <- model(it)
ptm <- model(pt)
rtm <- model(rt)
series <- c("ct", "(p.value)", "it", "(p.value)", "pt", "(p.value)", "rt", "(p.value)")
mtable <- rbind(ctm, itm, ptm, rtm)
mtable <- cbind(series, mtable)
kable(mtable, align = "l", caption = "OLS estimates for each series, p.value is shown in the bracket below each estimates, most of them are insignificant based on 5% significance level", padding = 2)
```


##Question 2  

```{r functions, echo=FALSE, warning=FALSE}

tsdata <- tibble(ct, it, pt, rt)
tsdata <- ts(tsdata, start=c(1947,1), frequency = 4)

trend_test <- function(data){
data <-  na.omit(data)
adf <- adf.test(data, alternative = "explosive")
adf_p <- unname(adf$p.value) %>% round(digits = 2)
if (adf_p < 0.05) {
  adf_con <- "trend stationary"
} else {
  adf_con <- "unit root with drift"
}
adf <- tibble(adf_con, adf_p) 

pp <- pp.test(data, alternative = "explosive")
pp_p <- unname(pp$p.value) %>% round(digits = 2)
if (pp_p < 0.05) {
  pp_con <- "trend stationary"
} else {
  pp_con <- "unit root with drift"
}
pp <- tibble(pp_con, pp_p) 

kpss <- kpss.test(data, null='Trend')
kpss_p <- unname(kpss$p.value) %>% round(digits = 2)
if (kpss_p < 0.05) {
  kpss_con <- "unit root with drift"
} else {
  kpss_con <- "trend stationary"
}
kpss <- tibble(kpss_con, kpss_p)



ttable <- cbind(adf, pp, kpss) 
return(ttable)
}

stationary_test <- function(data){
data <- na.omit(data) 
adf <- adf.test(data, alternative = "stationary")
adf_p <- unname(adf$p.value) %>% round(digits = 2)
if (adf_p < 0.05) {
  adf_con <- "stationary"
} else {
  adf_con <- "unit root"
}
adf <- tibble(adf_con, adf_p) 

pp <- pp.test(data, alternative = "stationary")
pp_p <- unname(pp$p.value) %>% round(digits = 2)
if (pp_p < 0.05) {
  pp_con <- "stationary"
} else {
  pp_con <- "unit root"
}
pp <- tibble(pp_con, pp_p) 

kpss <- kpss.test(data, null='Level')
kpss_p <- unname(kpss$p.value) %>% round(digits = 2)
if (kpss_p < 0.05) {
  kpss_con <- "unit root"
} else {
  kpss_con <- "stationary"
}
kpss <- tibble(kpss_con, kpss_p)

ttable <- cbind(adf, pp, kpss) 
return(ttable)
}
```

To do unit root tests, we first need to determine whether there is trend or not. And for ADF test, we also need to find the proper lags. 

```{r ctplot, echo=FALSE, fig.cap="Time plot of four series", message=FALSE, warning=FALSE, fig.show="asis"}
library(gridExtra)
ctp <- autoplot(tsdata[,1], xlab = "time", ylab = "ct")
itp <- autoplot(tsdata[,2], xlab = "time", ylab = "it")
ptp <- autoplot(tsdata[,3], xlab = "time", ylab = "pt")
rtp <- autoplot(tsdata[,4], xlab = "time", ylab = "rt")
grid.arrange(ctp, itp, ptp, rtp, ncol=2)
```

Therefore, we plot each series to check the trend component. As shown in the figure 1, there are clear trend in "ct", "it" and "pt"; but no trend in "rt". So the hypothesis tests for "ct", "it" and "pt" is 
$$H_0: this \ series\  has\  a\  unit\  root\  with\  drift \ (unit\ root\ with\ drift)$$ 
$$H_a: this \ series\ has\ a\ time\ trend\ but\ no\ unit\ root \ (trend \ stationary)$$
The hypothesis test for "rt" is 
$$H_0: this \ series\  has\  a\  unit\  root \ (unit\ root)$$ 
$$H_a: this \ series\ is\ stationary\ (stationary)$$

As for proper lags, we let "adf.test" function in R to automatically choose one, and it chooses 6, 6, 6, 5 for ct, it, pt, and rt respectively. Then we use Durbin-Watson test to test serial correlation in the associated four sets of residuals. All four p-values are not showing enough evidence to reject the null. So we think the lags chosen by "adf.test" are good enough based on 5% significance level. We will use them for ADF test directly.

```{r experiment, echo=FALSE, message=FALSE, warning=FALSE}


ct_lag <- adf.test(ct)$parameter %>% unname() %>% round(.,digits = 0)
it_lag <- adf.test(it)$parameter %>% unname() %>% round(.,digits = 0)
pt_lag <- adf.test(pt)$parameter %>% unname() %>% round(.,digits = 0)
rt_lag <- na.omit(rt) %>% adf.test() %>% .$parameter %>% unname() %>% round(.,digits = 0)


lagtable <- tibble(ct_lag, it_lag, pt_lag, rt_lag) %>% rename(., "ct"=ct_lag, "it"=it_lag, "pt"=pt_lag, "rt"=rt_lag)


time <- 1:252

lag2ct <- lag(lag(ct))
lag3ct <- lag(lag2ct)
lag4ct <- lag(lag3ct)
lag5ct <- lag(lag4ct)
lag6ct <- lag(lag5ct)

lag2it <- lag(lag(it))
lag3it <- lag(lag2it)
lag4it <- lag(lag3it)
lag5it <- lag(lag4it)
lag6it <- lag(lag5it)

lag2pt <- lag(lag(pt))
lag3pt <- lag(lag2pt)
lag4pt <- lag(lag3pt)
lag5pt <- lag(lag4pt)
lag6pt <- lag(lag5pt)

lag2rt <- lag(lag(rt))
lag3rt <- lag(lag2rt)
lag4rt <- lag(lag3rt)
lag5rt <- lag(lag4rt)

pv_ct <- lm(ct~time+lag(ct)+lag2ct+lag3ct+lag4ct+lag5ct+lag6ct) %>% dwtest() %>% .$p.value %>% unname()
pv_it <- lm(it~time+lag(it)+lag2it+lag3it+lag4it+lag5it+lag6it) %>% dwtest() %>% .$p.value %>% unname()
pv_pt <- lm(pt~time+lag(pt)+lag2pt+lag3pt+lag4pt+lag5pt+lag6pt) %>% dwtest() %>% .$p.value %>% unname()
pv_rt <- lm(rt~time+lag(rt)+lag2rt+lag3rt+lag4rt+lag5rt) %>% dwtest() %>% .$p.value %>% unname()

pvtable <- tibble(pv_ct, pv_it, pv_pt, pv_rt) %>% rename(., "ct"=pv_ct, "it"=pv_it, "pt"=pv_pt, "rt"=pv_rt)

rbind(lagtable, pvtable) %>% cbind(c("lags", "p.value"), .) %>% format(digits=2) -> lptable
kable(lptable, col.names = c("", "ct", "it", "pt", "rt"), caption = "lags chosen by adf.test and p.value from Durbin-Watson test")

```


```{r tests, echo=FALSE, warning=FALSE}
ctt <- trend_test(ct)
itt <- trend_test(it)
ptt <- trend_test(pt)
rtt <- stationary_test(rt)

ttable <- rbind(c("conclusion", "p.value", "conclusion", "p.value", "conclusion","p.value"), ctt, itt, ptt, rtt)
series <- c("ct", "it","pt", "rt")
ttable <- cbind(c("", series), ttable)
kable(ttable, align = "l", col.names = c(" ", "ADF", "ADF", "PP", "PP", "KPSS", "KPSS"), caption = "Three unit root tests, ADF and PP always have same conclusions, KPSS makes two different decisions, all conclusions are based on 5% level")

```

From the table, we can see that ADF and PP always make same decisions for these four series at 5% significance level. Since there are no serial correlation left in the residuals for ADF test (given DW test), and the most important feature of PP test is to correct the calculation of standard deviation for the test statistic when there are serial correlation in the residuals, we expect ADF and PP to performs similarly in this case, so this result meets our expectation. What's more, KPSS differs from those two tests for "it" and "rt". Because KPSS test also expect the residuals to be i.i.d and it uses different lags with ADF test, so the residuals in KPSS test may not be i.i.d. With this possible violation of assumption, we think KPSS's conclusions are relatively more unreliable in this case.

Three out of four series fail to reject the unit root hypothesis. However, given the low power of the test against stationary but highly persistent alternatives, we cannot conclude that these three series do have unit root.

#Part II  

Part II is about @zivot2002.

##The main idea proposed  

@nelson1982 challenged the traditional view by arguing that most macroeconomic and financial aggregates cannot reject the unit root hypothesis using Dickey-Fuller test.

However, @perron1989 disagrees with Nelson and Plosser. He suggests that once the 1929 crash is taken as exogenous event and put into the regression, "for 11 out of the 14 series analyzed by Nelson and Plosser can be rejected at a high confidence level the unit root hypothesis". He also analyzes the postwar quarterly real GNP series using similar approach and again rejects the null hypothesis. 

Based on what @perron1989 did, @zivot2002 enter the debate using the same data set with @perron1989 but an altered test statistics. To be more specific, their paper argues that the "exogenous" assumption used by Perron is inappropriate. Instead, they think the breakpoint should be estimated but not fixed since it could be a realization from the tail of the underlying "true" distribution. In this paper, they talk about how to estimate the breakpoint. A new test statistic is created accounting for this consideration. Furthermore, they construct the asymptotic distribution and finite-sample distribution for this new test statistics. Finally, they compare their test results with Perron's. In addition, the effect of leptokurtosis and temporally dependent innovations is investigated. It turns out the test conclusion is sensitive to the assumption made by Perron. Once the "exogenous" assumption is relaxed and small sample bias is taken care of, most of the test conclusions will be changed.

##The main techniques used  

* Test regression  

The ADF (adjusted Dickey-Fuller) test is used for the unit root test, the test regression is estimated by OLS. Given different behavior of the data, three regression equations are used to test for a unit root. The first specification is designed to capture the change in level before and after the breakpoint; the second one is for the change in slope caused by the shock; the third one encompasses the first two scenarios.
$$y_t=\hat{\mu}^A+\hat{\theta}^A DU_t(\hat{\lambda})+\hat{\beta}^A t+\hat{\alpha}^Ay_{t-1}+\sum_{j=1}^k \hat{c_j}^A \Delta y_{t-j}+\hat{e_t}$$
$$y_t= \hat{\mu}^B +\hat{\gamma}^B DT_t^*(\hat{\lambda}) +\hat{\beta}^B t
+\hat{\alpha}^B y_{t-1}+\sum_{j=1}^k \hat{c_j}^B \Delta y_{t-j}+\hat{e_t}$$
$$y_t=\hat{\mu}^C+\hat{\theta}^C DU_t(\hat{\lambda})+\hat{\beta}^C t+\hat{\gamma}^C DT_t^*(\hat{\lambda})+\hat{\alpha}^C y_{t-1}+\sum_{j=1}^k \hat{c_j}^C \Delta y_{t-j}+\hat{e_t}$$
where $T\hat{\lambda}$ gives the estimated location of the breakpoint, the $\hat{\lambda}$ is chosen such that the one-sided t-statistic for testing $\alpha=1$ is minimized (in favor to the trend stationary alternative); $DU_t(\lambda)=1\ and\ DT_t^*(\lambda)=t-T\lambda$ only for observations after the breakpoint and both of them are equal to 0 otherwise. 

* Asymptotic distribution  

The asymptotic distribution is constructed by assuming standard Brownian motion for the minimum t-statistic. 
$$\underset{\lambda\in\Lambda}{inf}\ \ t_{\hat{\alpha}^i}(\lambda) \ \underset{\rightarrow}{d} \ \ \underset{\lambda\in\Lambda}{inf}\ \bigg( \int_0^1 W^i(\lambda , r)^2 dr \bigg)^{-1/2}
\times 
\bigg( \int_0^1 W^i(\lambda , r) dW(r) \bigg)\ as\ T \rightarrow \infty\ \forall i=A,\ B,\ C$$
To eliminate possible nuisance-parameter dependency problem, extra lags of first differences of the data are employed as regressors (which is represented by $\sum_{j=1}^k \hat{c_j}^i \Delta y_{t-j}$ in the regression). The number of lags are determined by working backward from $k=\bar{k}$ and choosing the first k such that the t-statistic is greater than 1.6 in absolute value (with $\bar{k}=8$ for Nelson and Plosser, $\bar{k}=12$ for postwar series). 
As to the critical values, simulation methods are used to get an approximation of the integral.

* Finite-sample distribution  

Under the null hypothesis, the model is 
$$y_t=\mu+y_{t-1}+e_t$$
Suppose $e_t \sim ARMA(p, q)$ then, $y_t-y_{t-1} \sim ARMA(p, q)$ as well, therefore, the estimated $\hat{p}$ and $\hat{q}$ can be obtained by fitting the best ARMA model to the first difference of $y_t$. Treating these estimated $\hat{p}$ and $\hat{q}$ as part of the true data generating process, Monte Carlo methods can be used to compute the exact finite-sample distribution of the test statistics. 

##The main data used  

Two data sets are used in this paper, the same with what Perron used in his paper.

* The Nelson and Plosser data.

It is the U.S. historical time series which include measures of output, spending, money, prices, and interest rates (total 13 series). The data are annual, generally averages for the year, with starting dates from 1860 to 1909 and ending in 1970 in all cases. All series except interest rate are transformed to natural logs. [@nelson1982]

* The postwar quarterly real GNP series extracted from the Citibase data bank.

The data is from 1947:I to 1986:III and so contains only one break as well, the 1973 oil shock. The 1973 oil price shock did not cause a significant drop in the level of the series. However, after that date, the slope of the trend function has sensibly decreased. The data is transformed to natural logs. [@perron1989]  

##The main results obtained  

* Distribution of t-statistic

Two of the primary results in this paper are the limiting distribution and finite-sample distribution of the t-statistic. From the plot given by @zivot2002 (figure 1), we can see from the left to the right are finite-sample distribution, limiting distribution and Perron's distribution respectively. Therefore, for a left-tailed test, it is the most difficult to reject the null under the finite-sample distribution and easiest under Perron's.

* Test results

The majority of the t-statistics obtained in this paper are bigger (more in favor to the alternative hypothesis) than Perron's in that the test regression does not include the dummy variable, $D(T_B)_t=1$ only when $t=T_B+1$. Also the estimated breakpoint differs with Perron's for the postwar quarterly real GNP. 

Under the asymptotic distribution, @zivot2002 fail to reject 5 out of 11 series in Nelson and Plosser data set which was rejected by @perron1989 at 5% significance level, the postwar quarterly real GNP is not rejected as well. 

Under the finite-sample distribution, 3 more "Perron rejected" series in Nelson and Plosser data set are not rejected anymore. Table below shows the comparison between Zivot and Perron's test conclusion based on 5% level.

```{r table, echo=FALSE}
tests <- c("Perron", "Zivot asymptotic", "Zivot finite-sample")
names(tests) <- c(1,2,3)
rejection <- c("11","6", "3")
names(rejection) <-c(1,2,3)
compare_table <- rbind(tests, rejection)
kable(compare_table, col.names = c("", "", ""), caption = "Comparison of number of rejections made by different tests, out of 14 series, all based on 5% level")
```

##The conclusions made in the paper  

Although the evidence of the unit root hypothesis being rejected by the majority of the series is weakened, after endogenizing the breakpoint selection and correcting for small sample bias. However, it does not necessarily mean that unit root hypothesis being accepted. Because the low power of the test against trend stationary with broken trend series. 
On the other hand, there are still three series being rejected by @zivot2002, these series passed the toughest test, we can say there are stronger evidence for them not being unit root.

##Some Discussion  

Following the estimates in table 6 in @zivot2002, I simulated two series for the "consumer prices" as shown in figure 2. One is generated under the null hypothesis while the other one is under the alternative. Can you tell which one is which?

```{r simplot, echo=FALSE, fig.cap="Two simulated series for consumer prices using estimates from table 6 in the article under discussion, the first one is generated under the null hypothesis while the second one is under the alternative.", fig.show="asis", message=FALSE, warning=FALSE}
year<-1863:1973
yt_null <- vector()
yt_1 <- 0
set.seed(4)
for (t in 1:111) {
  yt_null[t] <- 0.217+ yt_1 +rnorm(1, 0, 1)
  yt_1 <- yt_null[t]
}
yt_null <- as.ts(yt_null)
pnull <- autoplot(yt_null, xlab="year", ylab="simulated consumer prices")

yt_al <- vector()
d0 <- rep(0, 10)
d1 <- rep(1, 101)
du <- c(d0, d1)
yt_1 <-0

for (t in 1:111) {
  yt_al[t] <- 0.217 - 0.055*du[t]+0.001*year[t]+ 0.941*yt_1+rnorm(1, 0, 1)
  yt_1 <- yt_al[t]
}
yt_al <- as.ts(yt_al)
pal <- autoplot(yt_al, xlab="year", ylab="simulated consumer prices")

grid.arrange(pnull, pal)
```

It is already a difficult task to distinguish a unit root with drift from a highly presistent trend stationary process. It certainly will be much more difficult to distinguish the former from a stationary one with a broken trend. All three studies @perron1989, @nelson1982, @zivot2002 are inspiring and stimulating. 
As @perron1989 said, "whichever view one adopts cannot be decided by data alone". It is reasonable to consider what happens in the real world and make decisions about how we should use all those relevant information. However, Perron believes the great crash and oil shock are exogenous to the data generating process which seems not realistic since political or other types of events are often related to the economic growth. @zivot2002 process takes care of this endogeneity. Although their methods are creative and useful, however, it seems to be purely data manipulation.
Therefore, I think a better approach is a combination of @perron1989 and @zivot2002. 
First one should study the data thoroughly, determine if there is possible breaks in the trend, how many breaks there are, if there are any big events happening near the skeptical breaks (war, financial crisis, etc). Then estimated locations of the breaks can be obtained by different methods (minimize/maximize the test statistic or maximize test power, etc). 

#Reference  






















